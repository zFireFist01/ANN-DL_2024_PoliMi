{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7DkPT6mHsng"
   },
   "source": [
    "### Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lou4UhZwHsnh"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7QaYBzwHsnh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    # Clear VRAM\n",
    "    tf.keras.backend.clear_session()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "    # Clear RAM\n",
    "    gc.collect()\n",
    "\n",
    "#This should clear the VRAM and RAM\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4yVdMieF0vz"
   },
   "source": [
    "### Import the Datasets in my drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21k10f0bFxD4",
    "outputId": "29f61403-2a71-43bc-de2b-bfc1a2a6b1c3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXiKBamcFwxN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "training_set_path = '/content/drive/My Drive/[2024-2025] AN2DL/Homework 1'\n",
    "folder_path = '/content/drive/My Drive/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzABAuZdHsnh"
   },
   "source": [
    "### Check GPU Existence and Status\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akAHd3rhHsni",
    "outputId": "9fa30384-fd77-47af-d993-ba3b4f6b185d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs TensorFlow detects\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow detected the following GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Name: {details['device_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwfPKyP2Hsni",
    "outputId": "c1aac6ea-50d8-4e60-da77-8aa797a92195"
   },
   "outputs": [],
   "source": [
    "#This is to check GPU-Status and Usage (works only for NVIDIA GPUs)\n",
    "!nvidia-smi\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg9e8t0JHsni"
   },
   "source": [
    "### Check Tensorflow and Keras Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7XmqVHmHsni"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klXZpkQSHsnj"
   },
   "source": [
    "### Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcveFXEmHsnj",
    "outputId": "acd43303-edd9-42e7-f4cd-d8a67f0b5d40"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "from keras import regularizers\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(tf.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import keras_cv\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline\n",
    "\n",
    "#Number of Classes in the Dataset\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1z_lx0dHsnj"
   },
   "source": [
    "### Create a function to Load Data and load the datasets needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkkVHbMCEeGT"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Load dataset from .npz file\n",
    "    data = np.load(path)\n",
    "\n",
    "    # Trim dataset to the first 11959 entries and discard the rest since they are garbage data\n",
    "    train_dataset = data['images'][:11959].copy()  # Copy to ensure no reference to the original array\n",
    "    test_dataset = data['labels'][:11959].copy()\n",
    "\n",
    "    # Explicitly delete the original data to free up memory\n",
    "    del data\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiBzonbZEeGT",
    "outputId": "a64366e8-d679-4885-e31a-092303de2d06"
   },
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X, y) = load_data(\"training_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X.shape)\n",
    "print(\"Test set shape (labels):\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug, y_test_aug) = load_data(\"augmented_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug.shape)\n",
    "\n",
    "#One-hot encoding\n",
    "y_test_aug = tfk.utils.to_categorical(y_test_aug3, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug3, y_test_aug3) = load_data(\"augmented_set3.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug3.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug3.shape)\n",
    "\n",
    "#One-hot encoding\n",
    "y_test_aug3 = tfk.utils.to_categorical(y_test_aug3, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug4, y_test_aug4) = load_data(\"augmented_set4.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug4.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug4.shape)\n",
    "\n",
    "#One-hot encoding\n",
    "y_test_aug4 = tfk.utils.to_categorical(y_test_aug4, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK4uuAwZHsnk"
   },
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo le proporzioni\n",
    "train_size = 1.0 # 70% training\n",
    "val_size = 1.0  # 15% validation\n",
    "test_size = 0.30  # 15% test\n",
    "\n",
    "# Calcoliamo gli indici di split\n",
    "total_samples = len(X)\n",
    "train_samples = int(total_samples * train_size)\n",
    "val_samples = int(total_samples * val_size)\n",
    "\n",
    "# Dividiamo i dati\n",
    "X_train = X[:train_samples]\n",
    "y_train = y[:train_samples]\n",
    "\n",
    "X_val = X[:val_samples]\n",
    "y_val = y[:val_samples]\n",
    "\n",
    "X_test = X[train_samples + val_samples:]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "# Liberiamo memoria\n",
    "del X, y\n",
    "gc.collect()\n",
    "\n",
    "# Stampiamo le dimensioni per verifica\n",
    "print(f\"Training set shape: {X_train.shape} - {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape} - {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape} - {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBcsepkVHsnl"
   },
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding usando keras\n",
    "y_train = tfk.utils.to_categorical(y_train, num_classes=8)\n",
    "y_val = tfk.utils.to_categorical(y_val, num_classes=8)\n",
    "y_test = tfk.utils.to_categorical(y_test, num_classes=8)\n",
    "\n",
    "# Stampiamo le dimensioni per verifica\n",
    "print(f\"Training labels shape after one-hot encoding: {y_train.shape}\")\n",
    "print(f\"Validation labels shape after one-hot encoding: {y_val.shape}\")\n",
    "print(f\"Test labels shape after one-hot encoding: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Class Distribution in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola le class weights dalle y_train (prima di convertirle in one-hot)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ottieni le etichette originali dalle one-hot encoded\n",
    "y_train_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Calcola i class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "\n",
    "# Crea un dizionario di class weights\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Stampa le informazioni\n",
    "print(\"Class distribution:\")\n",
    "for class_idx, count in enumerate(np.bincount(y_train_labels)):\n",
    "    print(f\"Class {class_idx}: {count} samples, weight = {class_weights_dict[class_idx]:.3f}\")\n",
    "\n",
    "# Visualizza anche in percentuale\n",
    "total_samples = len(y_train_labels)\n",
    "print(\"\\nClass distribution (percentage):\")\n",
    "for class_idx, count in enumerate(np.bincount(y_train_labels)):\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"Class {class_idx}: {percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7wYjeXNHsnl"
   },
   "source": [
    "### Define The First - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7m8B6pNWHsnl",
    "outputId": "c6698fb1-2a2b-4a79-8207-1c9d84212f3c"
   },
   "outputs": [],
   "source": [
    "# Initialise ConvNeXtXLarge model with pretrained weights, for transfer learning\n",
    "convnext =  tf.keras.applications.ConvNeXtXLarge(\n",
    "    include_top=False,             # Esclude il classificatore finale\n",
    "    input_shape=(96, 96, 3),       # Dimensioni di input\n",
    "    weights=\"imagenet\",            # Pesi preaddestrati su ImageNet\n",
    "    input_tensor=None,             # Tensor di input (lascia None per usare input_shape)\n",
    "    pooling=None,                  # Nessun pooling; specifica 'avg' per GlobalAveragePooling\n",
    "    classes=8,                     # Numero di classi (non usato se include_top=False)\n",
    "    classifier_activation=\"softmax\" # Attivazione del classificatore (non usato se include_top=False)\n",
    ")\n",
    "\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "convnext.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "# Specify 'to_file' argument with a path where you have write permissions\n",
    "tfk.utils.plot_model(convnext, to_file='/tmp/model.png', expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G87Q1ZhDHsnl",
    "outputId": "db4f07b9-486a-409d-bde7-14bd93c57031"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers in ConvNeXtXLarge to use it solely as a feature extractor\n",
    "convnext.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "\n",
    "# We wanted to try this approach, so also adding an augmentation pipeline to the model insted of using pre augmented data\n",
    "augmentation = keras_cv.layers.Augmenter([\n",
    "    keras_cv.layers.AutoContrast(value_range=(0, 255)),\n",
    "    keras_cv.layers.AugMix(severity=0.5, value_range=(0, 255)),\n",
    "    keras_cv.layers.ChannelShuffle(),\n",
    "    #keras_cv.layers.CutMix(),\n",
    "    #keras_cv.layers.MixUp(),\n",
    "    #keras_cv.layers.JitteredResize(target_size=(96, 96), scale_factor=(1.0, 1.2)),\n",
    "    keras_cv.layers.RandAugment(magnitude=0.3, value_range=(0, 255)),\n",
    "    keras_cv.layers.RandomAugmentationPipeline(\n",
    "        layers=[\n",
    "            keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.1),\n",
    "            keras_cv.layers.RandomColorDegeneration(factor=0.3),\n",
    "            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
    "            keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 255)),\n",
    "            keras_cv.layers.RandomSaturation(factor=0.2),\n",
    "            keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0, 255)),\n",
    "            keras_cv.layers.RandomShear(x_factor=0.15, y_factor=0.15),\n",
    "            keras_cv.layers.Solarization(value_range=(0, 255))\n",
    "        ],\n",
    "        #augmentations_per_image=2\n",
    "        augmentations_per_image=tf.squeeze(tf.random.categorical(\n",
    "            tf.math.log([[0.1, 0.2, 0.4, 0.3]]),  # [0 layers: 10%, 1 layer: 20%, 2 layers: 40%, 3 layers: 30%]\n",
    "            1\n",
    "        ))\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "#Apply the augmentation pipeline\n",
    "inputs = augmentation(inputs)\n",
    "\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = convnext(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "#x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "\n",
    "# Add layer normalizatiFinal_Project.ipynbon\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Lion optimiser\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer= tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "tl_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnDRduxMHsnl"
   },
   "source": [
    "### Train First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hg7D7J2NHsnm",
    "outputId": "042d236f-4c3c-45e9-aded-de19abc6d94a"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tl_history = tl_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=30,\n",
    "    epochs=20,\n",
    "    class_weight=class_weights_dict,\n",
    "    validation_data=(X_val , y_val),\n",
    "    callbacks=[tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ]\n",
    ").history\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "#model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
    "#tl_model.save(model_filename)\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras.weights.h5'\n",
    "tl_model.save_weights(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvfy_vjwHsnn"
   },
   "source": [
    "### Test the First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knaoCC7MHsnn",
    "outputId": "f872dea7-9e07-4f76-ae98-a56ccef15900"
   },
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = tl_model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "del tl_model\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDKxWaefHsnn"
   },
   "source": [
    "### First Fine - Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
    "mobilenetf = tf.keras.applications.ConvNeXtXLarge(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=False,\n",
    "    classes=8,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenetf.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "\n",
    "# Definisci il pipeline completo di augmentazione\n",
    "augmentation = keras_cv.layers.Augmenter([\n",
    "    keras_cv.layers.AutoContrast(value_range=(0, 255)),\n",
    "    keras_cv.layers.AugMix(severity=0.5, value_range=(0, 255)),\n",
    "    keras_cv.layers.ChannelShuffle(),\n",
    "    #keras_cv.layers.CutMix(),\n",
    "    #keras_cv.layers.MixUp(),\n",
    "    #keras_cv.layers.JitteredResize(target_size=(96, 96), scale_factor=(1.0, 1.2)),\n",
    "    keras_cv.layers.RandAugment(magnitude=0.3, value_range=(0, 255)),\n",
    "    keras_cv.layers.RandomAugmentationPipeline(\n",
    "        layers=[\n",
    "            keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.1),\n",
    "            keras_cv.layers.RandomColorDegeneration(factor=0.3),\n",
    "            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
    "            keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 255)),\n",
    "            keras_cv.layers.RandomSaturation(factor=0.2),\n",
    "            keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0, 255)),\n",
    "            keras_cv.layers.RandomShear(x_factor=0.15, y_factor=0.15),\n",
    "            keras_cv.layers.Solarization(value_range=(0, 255))\n",
    "        ],\n",
    "        #augmentations_per_image=2\n",
    "        augmentations_per_image=tf.squeeze(tf.random.categorical(\n",
    "            tf.math.log([[0.1, 0.2, 0.4, 0.3]]),  # [0 layers: 10%, 1 layer: 20%, 2 layers: 40%, 3 layers: 30%]\n",
    "            1\n",
    "        ))\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "#Apply the augmentation pipeline\n",
    "inputs = augmentation(inputs)\n",
    "\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenetf(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "#x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "\n",
    "# Add layer normalizatiFinal_Project.ipynbon\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "ft_model  = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model .compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Load the saved weights\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_80.15.keras.weights.h5'  # replace <final_val_accuracy> with the actual accuracy\n",
    "ft_model.load_weights(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UfK70LvCHsnn",
    "outputId": "5d07c10c-710f-4bed-d82c-769215a9cece"
   },
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "#ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_85.5.keras')\n",
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6OBf5tXHsnn",
    "outputId": "67188052-c2c4-47a9-ac90-6a8965ac603a"
   },
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "ft_model.get_layer('convnext_xlarge').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in ft_model.get_layer('convnext_xlarge').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aomp12LRHsnn",
    "outputId": "1332da9a-1c27-4bcc-8f72-871bbff6acd1"
   },
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 124\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7Ow8u8vHsnn"
   },
   "outputs": [],
   "source": [
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7_xqnKPHsnn",
    "outputId": "4640bda3-cf92-4a0d-d38e-4c6fa36aa052"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = X,          # Corretto: usa training set\n",
    "    y = y,          # Corretto: usa training set\n",
    "    batch_size = 32,\n",
    "    epochs = 30,          # Aumentato per dare pi√π tempo al training\n",
    "    validation_data = (X, y),\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            patience=7,    # Aumentato per evitare stop prematuro\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,    # Aumentato ma mantenuto minore dell'EarlyStopping\n",
    "            min_lr=1e-7,\n",
    "            verbose=1,\n",
    "            mode='min',\n",
    "            min_delta=1e-4,\n",
    "            cooldown=1\n",
    "        )\n",
    "    ]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "ft_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxRd-Pn7Hsnn"
   },
   "source": [
    "### Second Fine - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8PcCtEoBHsnn",
    "outputId": "f1c706a9-7e9e-48eb-8646-ea76ac37a8ea"
   },
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "#ft_model = tfk.models.load_model('/content/Blood_Cells_MobileNetV3S_75.08.keras')\n",
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkXX9nrSHsnn",
    "outputId": "a7c088cc-7fad-482f-aa19-58126bd4183e"
   },
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "ft_model.get_layer('convnext_xlarge').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in ft_model.get_layer('convnext_xlarge').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YFgJNcLhHsnn",
    "outputId": "c72f82af-c2eb-48d5-995a-ac54435af745"
   },
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 100\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmqkBOPrHsnn"
   },
   "outputs": [],
   "source": [
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBlgqDz-Hsnn"
   },
   "outputs": [],
   "source": [
    "# Enable mixed precision\n",
    "tfk.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "kMSjUmUoHsnn",
    "outputId": "cf7c6f70-7d2b-4a36-cd69-ec22b5fd680d"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 5,\n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=False),\n",
    "                 tfk.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,        # Riduzione graduale per un modello grande\n",
    "                    patience=1,        # ~25-30% delle epoche totali\n",
    "                    min_lr=1e-7,      # Considerando Adam come optimizer\n",
    "                    verbose=1,\n",
    "                    mode='min',\n",
    "                    min_delta=1e-4,   # Basato sulla scala delle tue loss\n",
    "                    cooldown=1        # Breve periodo di stabilizzazione\n",
    ")]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "ft_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Second Fine - Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test_aug)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test_aug2)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug2.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test_aug3)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test_aug4)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMEcNOVsHsno"
   },
   "source": [
    "### Submit Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgrkr0cVHsno"
   },
   "outputs": [],
   "source": [
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRs3RtsQHsno",
    "outputId": "f95efc23-2cd8-4a1a-9a7d-ceb21b1219c7"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('Blood_Cells_MobileNetV3S_99.12.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nse5OExwHsno",
    "outputId": "51050626-80eb-4685-b6dc-3f92a5e520fc"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "# The original path was incorrect. Using f-string to format correctly.\n",
    "!zip {filename} model.py Blood_Cells_MobileNetV3S_99.12.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the submission file into the drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-oMJvTazOTX"
   },
   "outputs": [],
   "source": [
    "!cp submission_241116_200916.zip /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hzABAuZdHsnh",
    "Sg9e8t0JHsni",
    "klXZpkQSHsnj",
    "RBcsepkVHsnl",
    "a7wYjeXNHsnl",
    "PnDRduxMHsnl",
    "Vvfy_vjwHsnn",
    "VDKxWaefHsnn"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
