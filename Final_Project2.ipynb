{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    # Clear VRAM\n",
    "    tf.keras.backend.clear_session()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    \n",
    "    # Clear RAM\n",
    "    gc.collect()\n",
    "\n",
    "#This should clear the VRAM and RAM\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Existence and Status\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs TensorFlow detects\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow detected the following GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Name: {details['device_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to check GPU-Status and Usage (works only for NVIDIA GPUs)\n",
    "!nvidia-smi\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow and Keras Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "from keras import regularizers\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(tf.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline\n",
    "\n",
    "#Number of Classes in the Dataset\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to Load Data and load the datasets needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Load dataset from .npz file\n",
    "    data = np.load(path)\n",
    "    \n",
    "    # Trim dataset to the first 11959 entries and discard the rest\n",
    "    train_dataset = data['images'][:11959].copy()  # Copy to ensure no reference to the original array\n",
    "    test_dataset = data['labels'][:11959].copy()\n",
    "    \n",
    "    # Explicitly delete the original data to free up memory\n",
    "    del data\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test, y_test) = load_data(\"training_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test.shape)\n",
    "print(\"Test set shape (labels):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug, y_test_aug) = load_data(\"augmented_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug2, y_test_aug2) = load_data(\"augmented_set2.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug2.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug3, y_test_aug3) = load_data(\"augmented_set3.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug3.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape (images): (11959, 96, 96, 3)\n",
      "Test set shape (labels): (11959, 1)\n"
     ]
    }
   ],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug4, y_test_aug4) = load_data(\"augmented_set4.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug4.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft2, fftshift, ifft2\n",
    "import cv2\n",
    "\n",
    "# Assuming the data has been loaded into variables\n",
    "# (X_test_aug4, y_test_aug4) = load_data(\"augmented_set4.npz\")\n",
    "\n",
    "# Function to create an adaptive high-pass filter based on the frequency spectrum\n",
    "def adaptive_high_pass_filter(f_transform, threshold_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Generates an adaptive high-pass filter mask based on the frequency spectrum.\n",
    "\n",
    "    Parameters:\n",
    "    - f_transform: The Fourier transform of the image.\n",
    "    - threshold_ratio: Ratio of the average magnitude to set as the cutoff.\n",
    "\n",
    "    Returns:\n",
    "    - mask: High-pass filter mask.\n",
    "    \"\"\"\n",
    "    # Compute magnitude spectrum\n",
    "    magnitude_spectrum = np.abs(f_transform)\n",
    "    \n",
    "    # Compute adaptive threshold based on mean magnitude\n",
    "    adaptive_threshold = magnitude_spectrum.mean() * threshold_ratio\n",
    "    \n",
    "    # Create a mask where frequencies below the threshold are set to 0\n",
    "    mask = magnitude_spectrum > adaptive_threshold\n",
    "    return mask\n",
    "\n",
    "# Process each image in the dataset in-place\n",
    "for i, image in enumerate(X_test_aug4):\n",
    "    # Ensure image is in uint8 format if needed\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "    # Process each color channel separately for color images\n",
    "    if len(image.shape) == 3:  # If image has color channels\n",
    "        filtered_channels = []\n",
    "        \n",
    "        for channel in cv2.split(image):\n",
    "            # Fourier transform for the channel\n",
    "            f_transform = fft2(channel)\n",
    "            f_transform_shifted = fftshift(f_transform)\n",
    "            \n",
    "            # Apply adaptive high-pass filter\n",
    "            adaptive_filter = adaptive_high_pass_filter(f_transform_shifted, threshold_ratio=0.2)\n",
    "            filtered_transform = f_transform_shifted * adaptive_filter\n",
    "            \n",
    "            # Convert back to spatial domain\n",
    "            filtered_channel = np.abs(ifft2(np.fft.ifftshift(filtered_transform)))\n",
    "            filtered_channel = cv2.normalize(filtered_channel, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            \n",
    "            # Append the filtered channel\n",
    "            filtered_channels.append(filtered_channel)\n",
    "        \n",
    "        # Merge channels back into a color image and update in-place\n",
    "        X_test_aug4[i] = cv2.merge(filtered_channels)\n",
    "    \n",
    "    else:  # Grayscale image\n",
    "        # Perform Fourier transform\n",
    "        f_transform = fft2(image)\n",
    "        f_transform_shifted = fftshift(f_transform)\n",
    "        \n",
    "        # Apply adaptive high-pass filter\n",
    "        adaptive_filter = adaptive_high_pass_filter(f_transform_shifted, threshold_ratio=0.2)\n",
    "        filtered_transform = f_transform_shifted * adaptive_filter\n",
    "        \n",
    "        # Convert back to spatial domain and normalize\n",
    "        filtered_image = np.abs(ifft2(np.fft.ifftshift(filtered_transform)))\n",
    "        X_test_aug4[i] = cv2.normalize(filtered_image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Combination of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different type of concatenations of the datasets\n",
    "X_test_concat = np.concatenate((X_test, X_test_aug), axis=0)\n",
    "y_test_concat = np.concatenate((y_test, y_test_aug), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets \n",
    "X_test_concat2 = np.concatenate((X_test_concat, X_test_aug2), axis=0)\n",
    "y_test_concat2 = np.concatenate((y_test_concat, y_test_aug2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "X_test_concat3 = np.concatenate((X_test_concat, X_test_aug3), axis=0)\n",
    "y_test_concat3 = np.concatenate((y_test_concat, y_test_aug3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets -> All the datasets were made by me \n",
    "X_test_concat_d = np.concatenate((X_test_aug, X_test_aug3), axis=0)\n",
    "y_test_concat_d = np.concatenate((y_test_aug, y_test_aug3), axis=0)\n",
    "X_test_concat_d = np.concatenate((X_test_concat_d, X_test_aug4), axis=0)\n",
    "y_test_concat_d = np.concatenate((y_test_concat_d, y_test_aug4), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to conditionally one-hot encode a variable if it exists\n",
    "def conditional_one_hot_encode(var_name, num_classes):\n",
    "    if var_name in globals() and globals()[var_name] is not None:\n",
    "        globals()[var_name] = tfk.utils.to_categorical(globals()[var_name], num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Conditionally apply one-hot encoding to each variable\n",
    "conditional_one_hot_encode('y_test_aug', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug2', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug3', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug4', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat2', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat3', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat_d', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Ranger Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranger(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=1e-5, weight_decay=1e-4, sync_period=5, slow_step=0.5, name=\"Ranger\", **kwargs):\n",
    "        # Pass learning_rate directly to the base class initializer\n",
    "        super(Ranger, self).__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "        \n",
    "        # Store other parameters\n",
    "        self.weight_decay = weight_decay\n",
    "        self.sync_period = sync_period\n",
    "        self.slow_step = slow_step\n",
    "        self.iterations = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "        \n",
    "        # Lookahead slow weights initialization\n",
    "        self.slow_weights = None\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, name=None, **kwargs):\n",
    "        # Use self.learning_rate directly in apply_gradients\n",
    "        for grad, var in grads_and_vars:\n",
    "            if grad is not None:\n",
    "                if self.weight_decay > 0:\n",
    "                    grad += self.weight_decay * var\n",
    "                var.assign_sub(self.learning_rate * grad)\n",
    "\n",
    "        # Initialize slow weights on the first call\n",
    "        if self.slow_weights is None:\n",
    "            self.slow_weights = [tf.Variable(v, trainable=False) for _, v in grads_and_vars]\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        self.iterations.assign_add(1)\n",
    "\n",
    "        # Apply Lookahead every sync_period steps using tf.cond\n",
    "        def apply_lookahead():\n",
    "            for slow_var, (_, var) in zip(self.slow_weights, grads_and_vars):\n",
    "                slow_var.assign_add(self.slow_step * (var - slow_var))\n",
    "                var.assign(slow_var)\n",
    "            return tf.no_op()  # no-op to satisfy return type requirement\n",
    "\n",
    "        # Check if the current iteration is a sync period\n",
    "        tf.cond(tf.equal(self.iterations % self.sync_period, 0), apply_lookahead, lambda: tf.no_op())\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Ranger, self).get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"sync_period\": self.sync_period,\n",
    "            \"slow_step\": self.slow_step\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Instantiate and use the custom Ranger optimizer\n",
    "optimizer = Ranger(learning_rate=1e-5, weight_decay=1e-4, sync_period=5, slow_step=0.5)\n",
    "optimizer_Adam = tfk.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The First - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mobilenet \u001b[38;5;241m=\u001b[39m \u001b[43mtfk\u001b[49m\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mEfficientNetB4(\n\u001b[1;32m      3\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m      5\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     input_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      9\u001b[0m     classifier_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefficientnet-b4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display a summary of the model architecture\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mobilenet\u001b[38;5;241m.\u001b[39msummary(expand_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfk' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
    "mobilenet = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=False,\n",
    "    classes=8,\n",
    "    classifier_activation=\"softmax\",\n",
    "    name=\"efficientnetv2-b3\",\n",
    ")\n",
    "\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "mobilenet.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "# Specify 'to_file' argument with a path where you have write permissions\n",
    "tfk.utils.plot_model(mobilenet, to_file='/tmp/model.png', expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenet.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "\"\"\"\n",
    "# Definisci il pipeline completo di augmentazione\n",
    "augmentation = tf.keras.Sequential([\n",
    "    # Altre augmentazioni indipendenti\n",
    "    tfkl.RandomCrop(height=96, width=96),  # Regola la dimensione del crop se necessario\n",
    "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tfkl.RandomRotation(0.3),\n",
    "    tfkl.Dropout(0.1),\n",
    "    tfkl.Dropout(0.2),\n",
    "    tfkl.RandomContrast(0.3),\n",
    "    tfkl.RandomZoom(0.15),\n",
    "    tfkl.RandomBrightness(0.1),\n",
    "], name='advanced_preprocessing')\n",
    "\n",
    "\n",
    "#Apply the augmentation pipeline\n",
    "inputs = augmentation(inputs)\n",
    "\"\"\"\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenet(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dropout layer for regularization\n",
    "x = tfkl.Dropout(0.4, name='dropout')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "\n",
    "# Add layer normalizatiFinal_Project.ipynbon\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Lion optimiser\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=optimizer_Adam, metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "tl_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tl_history = tl_model.fit(\n",
    "    x=X_test_concat3,\n",
    "    y=y_test_concat3,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test_aug2 , y_test_aug2),\n",
    "    callbacks=[tfk.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        mode='max', patience=20,\n",
    "        restore_best_weights=True\n",
    "        )]\n",
    ").history\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "#model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
    "#tl_model.save(model_filename)\n",
    "\n",
    "# Save the trained model weights to a file with the accuracy included in the filename\n",
    "model_weights_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras.weights.h5'\n",
    "tl_model.save_weights(model_weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = tl_model.predict(X_test_aug3)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "del tl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Fine - Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_76.54.keras')\n",
    "#ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "ft_model.get_layer('efficientnetv2-b3').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in ft_model.get_layer('efficientnetv2-b3').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 100\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = X_test_concat_d,\n",
    "    y = y_test_concat_d,\n",
    "    batch_size = 32,\n",
    "    epochs = 6,\n",
    "    validation_data = (X_test_aug2, y_test_aug2),\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "ft_model.save(model_filename)\n",
    "\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Fine - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('/home/zfirefist/Desktop/ANNDL/Outputs/Best_Results/submission86%/Blood_Cells_MobileNetV3S_96.76.keras')\n",
    "#ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "ft_model.get_layer('efficientnetv2-b3').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in ft_model.get_layer('efficientnetv2-b3').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 40\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('efficientnetv2-b3').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the custom Ranger optimizer\n",
    "ft_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = X_test_aug3,\n",
    "    y = y_test_aug3,\n",
    "    batch_size = 32,\n",
    "    epochs = 15,\n",
    "    validation_data = (X_test_aug3, y_test_aug3),\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras.weights.h5'\n",
    "ft_model.save_weights(model_filename)\n",
    "\n",
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = ft_model.predict(X_test_aug3)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.13     11959\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.07     11959\n",
      "   macro avg       0.12      0.01      0.02     11959\n",
      "weighted avg       1.00      0.07      0.13     11959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_98.59.keras')\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "y_pred = ft_model.predict(X_test_aug4)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug4.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_predictions\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Apply TTA and predict\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m tta_y_pred_classes \u001b[38;5;241m=\u001b[39m \u001b[43mapply_tta_class_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_aug4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_augmentations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Evaluate predictions\u001b[39;00m\n\u001b[1;32m     51\u001b[0m y_test_classes \u001b[38;5;241m=\u001b[39m y_test_aug4\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mapply_tta_class_predictions\u001b[0;34m(model, test_images, num_augmentations)\u001b[0m\n\u001b[1;32m     33\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m test_images:\n\u001b[0;32m---> 35\u001b[0m     augmented_image \u001b[38;5;241m=\u001b[39m \u001b[43mtta_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m     augmented_images\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39msqueeze(augmented_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     38\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(augmented_images)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/augmenter.py:44\u001b[0m, in \u001b[0;36mAugmenter.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 44\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:436\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_augment(inputs), metadata)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, metadata\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage augmentation layers are expecting inputs to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank 3 (HWC) or 4D (NHWC) tensors. Got shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:543\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._batch_augment\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_augment\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:285\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._map_fn\u001b[0;34m(self, func, inputs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmap_fn(\n\u001b[1;32m    280\u001b[0m         func,\n\u001b[1;32m    281\u001b[0m         inputs,\n\u001b[1;32m    282\u001b[0m         fn_output_signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_output_signature(inputs),\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_vectorize:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorized_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmap_fn(func, inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:578\u001b[0m, in \u001b[0;36mvectorized_map\u001b[0;34m(fn, elems, fallback_to_while_loop, warn)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(static_first_dims)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_to_while_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback_to_while_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:228\u001b[0m, in \u001b[0;36mpfor\u001b[0;34m(loop_fn, iters, fallback_to_while_loop, parallel_iterations, warn)\u001b[0m\n\u001b[1;32m    225\u001b[0m     def_function\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    226\u001b[0m   f \u001b[38;5;241m=\u001b[39m def_function\u001b[38;5;241m.\u001b[39mfunction(f)\n\u001b[0;32m--> 228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m functions_run_eagerly \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m   def_function\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(functions_run_eagerly)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import keras_cv\n",
    "\n",
    "# Define a lightweight augmentation pipeline for TTA\n",
    "tta_augmentation = keras_cv.layers.Augmenter(\n",
    "    [\n",
    "        keras_cv.layers.AutoContrast(value_range=(0, 255)),\n",
    "        keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.05),\n",
    "        keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 255)),\n",
    "        keras_cv.layers.RandomSaturation(factor=0.1),\n",
    "        keras_cv.layers.RandomSharpness(factor=0.1, value_range=(0, 255)),\n",
    "        keras_cv.layers.Solarization(value_range=(0, 255)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Function to apply TTA and aggregate class predictions\n",
    "def apply_tta_class_predictions(model, test_images, num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Apply TTA and aggregate class predictions for the test set using majority voting.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        test_images: NumPy array of test images.\n",
    "        num_augmentations: Number of TTA augmentations per image.\n",
    "\n",
    "    Returns:\n",
    "        Final class predictions for the test set.\n",
    "    \"\"\"\n",
    "    tta_class_votes = []  # Collect class predictions from all augmentations\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        augmented_images = []\n",
    "        for image in test_images:\n",
    "            augmented_image = tta_augmentation({\"images\": tf.expand_dims(image, axis=0)})[\"images\"]\n",
    "            augmented_images.append(tf.squeeze(augmented_image, axis=0).numpy())\n",
    "        \n",
    "        augmented_images = np.array(augmented_images)\n",
    "        class_predictions = model.predict(augmented_images, batch_size=32).argmax(axis=1)  # Get predicted classes\n",
    "        tta_class_votes.append(class_predictions)\n",
    "\n",
    "    # Aggregate class votes (majority voting)\n",
    "    tta_class_votes = np.array(tta_class_votes)  # Shape: (num_augmentations, num_samples)\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tta_class_votes)\n",
    "    return final_predictions\n",
    "\n",
    "# Apply TTA and predict\n",
    "tta_y_pred_classes = apply_tta_class_predictions(ft_model, X_test_aug4, num_augmentations=5)\n",
    "\n",
    "# Evaluate predictions\n",
    "y_test_classes = y_test_aug4.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report with TTA (Majority Voting):\")\n",
    "print(classification_report(y_test_classes, tta_y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
    "mobilenetf = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=False,\n",
    "    classes=8,\n",
    "    classifier_activation=\"softmax\",\n",
    "    name=\"efficientnetv2-b3\",\n",
    ")\n",
    "\n",
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenetf.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenetf(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dropout layer for regularization\n",
    "x = tfkl.Dropout(0.4, name='dropout')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 un load_data(\"training_set.npz\")its and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tlf_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "tlf_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "\n",
    "# Load the saved weights\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras.weights.h5'  # replace <final_val_accuracy> with the actual accuracy\n",
    "tlf_model.load_weights(model_filename)\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "tlf_model.save(model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('Blood_Cells_MobileNetV3S_91.82.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'Outputs/submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "# The original path was incorrect. Using f-string to format correctly.\n",
    "!zip {filename} model.py Blood_Cells_MobileNetV3S_91.82.keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
