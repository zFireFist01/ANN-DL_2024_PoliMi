{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e7bE8-gBXHk",
        "outputId": "a93ac721-4adc-46e3-ea48-c46e630c603a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "#from tensorflow import keras as tfk\n",
        "import keras as tfk       #notice how I'm importing keras and not tensorflow.keras\n",
        "#from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
        "from keras import layers as tfkl\n",
        "import keras_cv\n",
        "\n",
        "\n",
        "print(f\"Tensorflow version -> {tf.__version__}\")\n",
        "print(f\"Keras version -> {tfk.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7DkPT6mHsng"
      },
      "source": [
        "### Clear GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lou4UhZwHsnh"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7QaYBzwHsnh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from numba import cuda\n",
        "import gc\n",
        "\n",
        "def clear_memory():\n",
        "    # Clear VRAM\n",
        "    tf.keras.backend.clear_session()\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "\n",
        "    # Clear RAM\n",
        "    gc.collect()\n",
        "\n",
        "#This should clear the VRAM and RAM\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4yVdMieF0vz"
      },
      "source": [
        "### Import the Datasets in my drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21k10f0bFxD4",
        "outputId": "29f61403-2a71-43bc-de2b-bfc1a2a6b1c3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXiKBamcFwxN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "training_set_path = '/content/drive/My Drive/[2024-2025] AN2DL/Homework 1'\n",
        "folder_path = '/content/drive/My Drive/Datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzABAuZdHsnh"
      },
      "source": [
        "### Check GPU Existence and Status\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAHd3rhHsni",
        "outputId": "9fa30384-fd77-47af-d993-ba3b4f6b185d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# List all GPUs TensorFlow detects\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"TensorFlow detected the following GPU(s):\")\n",
        "    for gpu in gpus:\n",
        "        details = tf.config.experimental.get_device_details(gpu)\n",
        "        print(f\"Name: {details['device_name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwfPKyP2Hsni",
        "outputId": "c1aac6ea-50d8-4e60-da77-8aa797a92195"
      },
      "outputs": [],
      "source": [
        "#This is to check GPU-Status and Usage (works only for NVIDIA GPUs)\n",
        "!nvidia-smi\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "for gpu in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg9e8t0JHsni"
      },
      "source": [
        "### Check Tensorflow and Keras Version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7XmqVHmHsni"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klXZpkQSHsnj"
      },
      "source": [
        "### Import all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcveFXEmHsnj",
        "outputId": "acd43303-edd9-42e7-f4cd-d8a67f0b5d40"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "import keras as tfk\n",
        "from keras import layers as tfkl\n",
        "from keras import regularizers\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import keras_cv\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "\n",
        "#Number of Classes in the Dataset\n",
        "num_classes = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1z_lx0dHsnj"
      },
      "source": [
        "### Create a function to Load Data and load the datasets needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XkkVHbMCEeGT"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    # Load dataset from .npz file\n",
        "    data = np.load(path)\n",
        "\n",
        "    # Trim dataset to the first 11959 entries and discard the rest\n",
        "    train_dataset = data['images'][:11959].copy()  # Copy to ensure no reference to the original array\n",
        "    test_dataset = data['labels'][:11959].copy()\n",
        "\n",
        "    # Explicitly delete the original data to free up memory\n",
        "    del data\n",
        "\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBzonbZEeGT",
        "outputId": "a64366e8-d679-4885-e31a-092303de2d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set shape (images): (11959, 96, 96, 3)\n",
            "Test set shape (labels): (11959, 1)\n"
          ]
        }
      ],
      "source": [
        "# Execute function and load data\n",
        "(X, y) = load_data(\"training_set.npz\")\n",
        "\n",
        "print(\"Test set shape (images):\", X.shape)\n",
        "print(\"Test set shape (labels):\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute function and load data\n",
        "(X_test_aug, y_test_aug) = load_data(\"augmented_set.npz\")\n",
        "\n",
        "print(\"Test set shape (images):\", X_test_aug.shape)\n",
        "print(\"Test set shape (labels):\", y_test_aug.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute function and load data\n",
        "(X_test_aug3, y_test_aug3) = load_data(\"augmented_set3.npz\")\n",
        "\n",
        "print(\"Test set shape (images):\", X_test_aug3.shape)\n",
        "print(\"Test set shape (labels):\", y_test_aug3.shape)\n",
        "\n",
        "#One-hot encoding\n",
        "y_test_aug = tfk.utils.to_categorical(y_test_aug3, num_classes=8)\n",
        "y_test_aug3 = tfk.utils.to_categorical(y_test_aug3, num_classes=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute function and load data\n",
        "(X_test_aug4, y_test_aug4) = load_data(\"augmented_set4.npz\")\n",
        "\n",
        "print(\"Test set shape (images):\", X_test_aug4.shape)\n",
        "print(\"Test set shape (labels):\", y_test_aug4.shape)\n",
        "\n",
        "#One-hot encoding\n",
        "y_test_aug4 = tfk.utils.to_categorical(y_test_aug4, num_classes=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK4uuAwZHsnk"
      },
      "source": [
        "### Different Combination of Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definiamo le proporzioni\n",
        "train_size = 1.0 # 70% training\n",
        "val_size = 1.0  # 15% validation\n",
        "test_size = 0.30  # 15% test\n",
        "\n",
        "# Calcoliamo gli indici di split\n",
        "total_samples = len(X)\n",
        "train_samples = int(total_samples * train_size)\n",
        "val_samples = int(total_samples * val_size)\n",
        "\n",
        "# Dividiamo i dati\n",
        "X_train = X[:train_samples]\n",
        "y_train = y[:train_samples]\n",
        "\n",
        "X_val = X[:val_samples]\n",
        "y_val = y[:val_samples]\n",
        "\n",
        "X_test = X[train_samples + val_samples:]\n",
        "y_test = y[train_samples + val_samples:]\n",
        "\n",
        "# Liberiamo memoria\n",
        "del X, y\n",
        "gc.collect()\n",
        "\n",
        "# Stampiamo le dimensioni per verifica\n",
        "print(f\"Training set shape: {X_train.shape} - {y_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape} - {y_val.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape} - {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBcsepkVHsnl"
      },
      "source": [
        "### One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = tfk.utils.to_categorical(y, num_classes=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding usando keras\n",
        "y_train = tfk.utils.to_categorical(y_train, num_classes=8)\n",
        "y_val = tfk.utils.to_categorical(y_val, num_classes=8)\n",
        "y_test = tfk.utils.to_categorical(y_test, num_classes=8)\n",
        "\n",
        "# Stampiamo le dimensioni per verifica\n",
        "print(f\"Training labels shape after one-hot encoding: {y_train.shape}\")\n",
        "print(f\"Validation labels shape after one-hot encoding: {y_val.shape}\")\n",
        "print(f\"Test labels shape after one-hot encoding: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            "Class 0: 11959 samples, weight = 1.000\n",
            "\n",
            "Class distribution (percentage):\n",
            "Class 0: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Calcola le class weights dalle y_train (prima di convertirle in one-hot)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Ottieni le etichette originali dalle one-hot encoded\n",
        "y_train_labels = np.argmax(y, axis=1)\n",
        "\n",
        "# Calcola i class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_labels),\n",
        "    y=y_train_labels\n",
        ")\n",
        "\n",
        "# Crea un dizionario di class weights\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Stampa le informazioni\n",
        "print(\"Class distribution:\")\n",
        "for class_idx, count in enumerate(np.bincount(y_train_labels)):\n",
        "    print(f\"Class {class_idx}: {count} samples, weight = {class_weights_dict[class_idx]:.3f}\")\n",
        "\n",
        "# Visualizza anche in percentuale\n",
        "total_samples = len(y_train_labels)\n",
        "print(\"\\nClass distribution (percentage):\")\n",
        "for class_idx, count in enumerate(np.bincount(y_train_labels)):\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"Class {class_idx}: {percentage:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7wYjeXNHsnl"
      },
      "source": [
        "### Define The First - Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7m8B6pNWHsnl",
        "outputId": "c6698fb1-2a2b-4a79-8207-1c9d84212f3c"
      },
      "outputs": [],
      "source": [
        "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
        "mobilenet =  tf.keras.applications.ConvNeXtXLarge(\n",
        "    include_top=False,             # Esclude il classificatore finale\n",
        "    input_shape=(96, 96, 3),       # Dimensioni di input\n",
        "    weights=None,                # Pesi preaddestrati su ImageNet\n",
        "    input_tensor=None,             # Tensor di input (lascia None per usare input_shape)\n",
        "    pooling=None,                  # Nessun pooling; specifica 'avg' per GlobalAveragePooling\n",
        "    classes=8,                     # Numero di classi (non usato se include_top=False)\n",
        "    classifier_activation=\"softmax\" # Attivazione del classificatore (non usato se include_top=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "mobilenet.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "# Specify 'to_file' argument with a path where you have write permissions\n",
        "tfk.utils.plot_model(mobilenet, to_file='/tmp/model.png', expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G87Q1ZhDHsnl",
        "outputId": "db4f07b9-486a-409d-bde7-14bd93c57031"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
        "mobilenet.trainable = False\n",
        "\n",
        "# Define input layer with shape matching the input images\n",
        "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
        "\n",
        "\n",
        "# Definisci il pipeline completo di augmentazione\n",
        "augmentation = keras_cv.layers.Augmenter([\n",
        "    keras_cv.layers.AutoContrast(value_range=(0, 255)),\n",
        "    keras_cv.layers.AugMix(severity=0.5, value_range=(0, 255)),\n",
        "    keras_cv.layers.ChannelShuffle(),\n",
        "    #keras_cv.layers.CutMix(),\n",
        "    #keras_cv.layers.MixUp(),\n",
        "    #keras_cv.layers.JitteredResize(target_size=(96, 96), scale_factor=(1.0, 1.2)),\n",
        "    keras_cv.layers.RandAugment(magnitude=0.3, value_range=(0, 255)),\n",
        "    keras_cv.layers.RandomAugmentationPipeline(\n",
        "        layers=[\n",
        "            keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.1),\n",
        "            keras_cv.layers.RandomColorDegeneration(factor=0.3),\n",
        "            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
        "            keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 255)),\n",
        "            keras_cv.layers.RandomSaturation(factor=0.2),\n",
        "            keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0, 255)),\n",
        "            keras_cv.layers.RandomShear(x_factor=0.15, y_factor=0.15),\n",
        "            keras_cv.layers.Solarization(value_range=(0, 255))\n",
        "        ],\n",
        "        #augmentations_per_image=2\n",
        "        augmentations_per_image=tf.squeeze(tf.random.categorical(\n",
        "            tf.math.log([[0.1, 0.2, 0.4, 0.3]]),  # [0 layers: 10%, 1 layer: 20%, 2 layers: 40%, 3 layers: 30%]\n",
        "            1\n",
        "        ))\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "#Apply the augmentation pipeline\n",
        "inputs = augmentation(inputs)\n",
        "\n",
        "\n",
        "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
        "x = mobilenet(inputs)\n",
        "\n",
        "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "\n",
        "# Add a batch normalization layer\n",
        "#x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
        "\n",
        "# Add a dense layer with 256 units and GELU activation\n",
        "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
        "\n",
        "\n",
        "# Add layer normalizatiFinal_Project.ipynbon\n",
        "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
        "\n",
        "# Add a second dense layer with 128 units and GELU activation\n",
        "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
        "\n",
        "# Add layer normalization\n",
        "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
        "\n",
        "# Add a third dense layer with 128 units and GELU activation\n",
        "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
        "'''\n",
        "# Add layer normalization\n",
        "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
        "'''\n",
        "# Add final Dense layer for classification with softmax activation\n",
        "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
        "\n",
        "# Define the complete model linking input and output\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss and Lion optimiser\n",
        "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer= tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "tl_model.summary(expand_nested=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnDRduxMHsnl"
      },
      "source": [
        "### Train First - Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hg7D7J2NHsnm",
        "outputId": "042d236f-4c3c-45e9-aded-de19abc6d94a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling Equalization.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Equalization.call():\n  • inputs=tf.Tensor(shape=(96, 96, 3), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(X, tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tl_history \u001b[38;5;241m=\u001b[39m \u001b[43mtl_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Passa sia le immagini che le etichette\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Target labels\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcooldown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:436\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_augment(inputs), metadata)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, metadata\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage augmentation layers are expecting inputs to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank 3 (HWC) or 4D (NHWC) tensors. Got shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:543\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._batch_augment\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_augment\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:286\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._map_fn\u001b[0;34m(self, func, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_vectorize:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mvectorized_map(func, inputs)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/base_image_augmentation_layer.py:484\u001b[0m, in \u001b[0;36mBaseImageAugmentationLayer._augment\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto_tensor()\n\u001b[1;32m    476\u001b[0m transformation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_random_transformation(\n\u001b[1;32m    477\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m    478\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m     segmentation_mask\u001b[38;5;241m=\u001b[39msegmentation_mask,\n\u001b[1;32m    482\u001b[0m )\n\u001b[0;32m--> 484\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    492\u001b[0m     image_ragged \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_output_dense_images\n\u001b[1;32m    493\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_output_ragged_images:\n\u001b[1;32m    494\u001b[0m     image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mRaggedTensor\u001b[38;5;241m.\u001b[39mfrom_tensor(image)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:340\u001b[0m, in \u001b[0;36mAugMix.augment_image\u001b[0;34m(self, image, transformation, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m result \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros_like(image)\n\u001b[1;32m    338\u001b[0m curr_chain \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 340\u001b[0m image, chain_mixing_weights, curr_chain, result \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_mixing_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mless\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurr_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_chains\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_on_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_mixing_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m result \u001b[38;5;241m=\u001b[39m weight_sample \u001b[38;5;241m*\u001b[39m image \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m weight_sample) \u001b[38;5;241m*\u001b[39m result\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:146\u001b[0m, in \u001b[0;36mAugMix._loop_on_width\u001b[0;34m(self, image, chain_mixing_weights, curr_chain, result)\u001b[0m\n\u001b[1;32m    143\u001b[0m chain_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_depth()\n\u001b[1;32m    145\u001b[0m depth_level \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 146\u001b[0m depth_level, image_aug \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdepth_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_aug\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mless\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_depth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_on_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepth_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_aug\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(chain_mixing_weights, curr_chain) \u001b[38;5;241m*\u001b[39m image_aug\n\u001b[1;32m    152\u001b[0m curr_chain \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:137\u001b[0m, in \u001b[0;36mAugMix._loop_on_depth\u001b[0;34m(self, depth_level, image_aug)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loop_on_depth\u001b[39m(\u001b[38;5;28mself\u001b[39m, depth_level, image_aug):\n\u001b[1;32m    134\u001b[0m     op_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_generator\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m    135\u001b[0m         shape\u001b[38;5;241m=\u001b[39m(), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m    136\u001b[0m     )\n\u001b[0;32m--> 137\u001b[0m     image_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     depth_level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m depth_level, image_aug\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:268\u001b[0m, in \u001b[0;36mAugMix._apply_op\u001b[0;34m(self, image, op_index)\u001b[0m\n\u001b[1;32m    262\u001b[0m augmented \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m    263\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    264\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_contrast(augmented),\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m augmented \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_equalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    274\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_posterize(augmented),\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    277\u001b[0m )\n\u001b[1;32m    278\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    279\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rotate(augmented),\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    282\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:270\u001b[0m, in \u001b[0;36mAugMix._apply_op.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m augmented \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m    263\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    264\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_contrast(augmented),\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    269\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_equalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    274\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_posterize(augmented),\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    277\u001b[0m )\n\u001b[1;32m    278\u001b[0m augmented \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    279\u001b[0m     op_index \u001b[38;5;241m==\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rotate(augmented),\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: augmented,\n\u001b[1;32m    282\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/aug_mix.py:159\u001b[0m, in \u001b[0;36mAugMix._equalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_equalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/vectorized_base_image_augmentation_layer.py:435\u001b[0m, in \u001b[0;36mVectorizedBaseImageAugmentationLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    432\u001b[0m images \u001b[38;5;241m=\u001b[39m inputs[IMAGES]\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, metadata\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage augmentation layers are expecting inputs to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank 3 (HWC) or 4D (NHWC) tensors. Got shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/vectorized_base_image_augmentation_layer.py:347\u001b[0m, in \u001b[0;36mVectorizedBaseImageAugmentationLayer._batch_augment\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmap_fn(\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_ragged_image_call,\n\u001b[1;32m    343\u001b[0m         inputs_for_raggeds,\n\u001b[1;32m    344\u001b[0m         fn_output_signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_ragged_image_signature(images),\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(images, tf\u001b[38;5;241m.\u001b[39mRaggedTensor)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_output_dense_images\n\u001b[1;32m    356\u001b[0m ):\n\u001b[1;32m    357\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto_tensor()\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/equalization.py:151\u001b[0m, in \u001b[0;36mEqualization.augment_images\u001b[0;34m(self, images, transformations, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mtransform_value_range(\n\u001b[1;32m    147\u001b[0m     images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_range, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    149\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 151\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequalize_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(images, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    157\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/equalization.py:152\u001b[0m, in \u001b[0;36mEqualization.augment_images.<locals>.<lambda>\u001b[0;34m(channel)\u001b[0m\n\u001b[1;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mtransform_value_range(\n\u001b[1;32m    147\u001b[0m     images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_range, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    149\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    151\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmap_fn(\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m channel: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequalize_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    153\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrange(tf\u001b[38;5;241m.\u001b[39mshape(images)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(images, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    157\u001b[0m images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras_cv/src/layers/preprocessing/equalization.py:65\u001b[0m, in \u001b[0;36mEqualization.equalize_channel\u001b[0;34m(self, images, channel_index)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mequalize_channel\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, channel_index):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"equalize_channel performs histogram equalization on a single channel.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        channel_index: channel to equalize\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     is_single_image \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     67\u001b[0m     images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, channel_index]\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Compute the histogram of the image channel.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# If the input is not a batch of images, directly using\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# tf.histogram_fixed_width is much faster than using tf.vectorized_map\u001b[39;00m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling Equalization.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Equalization.call():\n  • inputs=tf.Tensor(shape=(96, 96, 3), dtype=float32)"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "tl_history = tl_model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=30,\n",
        "    epochs=20,\n",
        "    class_weight=class_weights_dict,\n",
        "    validation_data=(X_val , y_val),\n",
        "    callbacks=[tfk.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            patience=20,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tfk.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ]\n",
        ").history\n",
        "\n",
        "# Calculate and print the best validation accuracy achieved\n",
        "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file, including final accuracy in the filename\n",
        "#model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
        "#tl_model.save(model_filename)\n",
        "\n",
        "# Save the trained model to a file, including final accuracy in the filename\n",
        "model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras.weights.h5'\n",
        "tl_model.save_weights(model_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "LRFThC8OZ2id",
        "outputId": "d186720f-b849-4cf6-a03f-b0d61c0a0ec7"
      },
      "outputs": [],
      "source": [
        "def clear_memory():\n",
        "    tf.keras.backend.clear_session()\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    gc.collect()\n",
        "\n",
        "    # Libera memoria GPU\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "\n",
        "# Pulisci la memoria prima di iniziare\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvfy_vjwHsnn"
      },
      "source": [
        "### Test the First - Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knaoCC7MHsnn",
        "outputId": "f872dea7-9e07-4f76-ae98-a56ccef15900"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = tl_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "del tl_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDKxWaefHsnn"
      },
      "source": [
        "### First Fine - Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
        "mobilenetf = tf.keras.applications.ConvNeXtXLarge(\n",
        "    include_top=False,\n",
        "    input_shape=(96, 96, 3),\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    pooling=False,\n",
        "    classes=8,\n",
        "    classifier_activation=\"softmax\"\n",
        ")\n",
        "\n",
        "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
        "mobilenetf.trainable = False\n",
        "\n",
        "# Define input layer with shape matching the input images\n",
        "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
        "\n",
        "\n",
        "# Definisci il pipeline completo di augmentazione\n",
        "augmentation = keras_cv.layers.Augmenter([\n",
        "    keras_cv.layers.AutoContrast(value_range=(0, 255)),\n",
        "    keras_cv.layers.AugMix(severity=0.5, value_range=(0, 255)),\n",
        "    keras_cv.layers.ChannelShuffle(),\n",
        "    #keras_cv.layers.CutMix(),\n",
        "    #keras_cv.layers.MixUp(),\n",
        "    #keras_cv.layers.JitteredResize(target_size=(96, 96), scale_factor=(1.0, 1.2)),\n",
        "    keras_cv.layers.RandAugment(magnitude=0.3, value_range=(0, 255)),\n",
        "    keras_cv.layers.RandomAugmentationPipeline(\n",
        "        layers=[\n",
        "            keras_cv.layers.RandomChannelShift(value_range=(0, 255), factor=0.1),\n",
        "            keras_cv.layers.RandomColorDegeneration(factor=0.3),\n",
        "            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
        "            keras_cv.layers.RandomHue(factor=0.1, value_range=(0, 255)),\n",
        "            keras_cv.layers.RandomSaturation(factor=0.2),\n",
        "            keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0, 255)),\n",
        "            keras_cv.layers.RandomShear(x_factor=0.15, y_factor=0.15),\n",
        "            keras_cv.layers.Solarization(value_range=(0, 255))\n",
        "        ],\n",
        "        #augmentations_per_image=2\n",
        "        augmentations_per_image=tf.squeeze(tf.random.categorical(\n",
        "            tf.math.log([[0.1, 0.2, 0.4, 0.3]]),  # [0 layers: 10%, 1 layer: 20%, 2 layers: 40%, 3 layers: 30%]\n",
        "            1\n",
        "        ))\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "#Apply the augmentation pipeline\n",
        "inputs = augmentation(inputs)\n",
        "\n",
        "\n",
        "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
        "x = mobilenetf(inputs)\n",
        "\n",
        "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "\n",
        "# Add a batch normalization layer\n",
        "#x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
        "\n",
        "# Add a dense layer with 256 units and GELU activation\n",
        "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
        "\n",
        "\n",
        "# Add layer normalizatiFinal_Project.ipynbon\n",
        "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
        "\n",
        "# Add a second dense layer with 128 units and GELU activation\n",
        "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
        "\n",
        "# Add layer normalization\n",
        "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
        "\n",
        "# Add a third dense layer with 128 units and GELU activation\n",
        "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
        "'''\n",
        "# Add layer normalization\n",
        "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
        "\n",
        "# Add another dropout layer\n",
        "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
        "'''\n",
        "# Add final Dense layer for classification with softmax activation\n",
        "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
        "\n",
        "\n",
        "# Define the complete model linking input and output\n",
        "ft_model  = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
        "ft_model .compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "\n",
        "# Load the saved weights\n",
        "model_filename = 'Blood_Cells_MobileNetV3S_80.15.keras.weights.h5'  # replace <final_val_accuracy> with the actual accuracy\n",
        "ft_model.load_weights(model_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UfK70LvCHsnn",
        "outputId": "5d07c10c-710f-4bed-d82c-769215a9cece"
      },
      "outputs": [],
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_85.5.keras')\n",
        "#ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6OBf5tXHsnn",
        "outputId": "67188052-c2c4-47a9-ac90-6a8965ac603a"
      },
      "outputs": [],
      "source": [
        "# Set the MobileNetV3Small model layers as trainable\n",
        "ft_model.get_layer('convnext_xlarge').trainable = True\n",
        "\n",
        "# Set all MobileNetV3Small layers as non-trainable\n",
        "for layer in ft_model.get_layer('convnext_xlarge').layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
        "        layer.trainable = True\n",
        "        print(i, layer.name, type(layer).__name__, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aomp12LRHsnn",
        "outputId": "1332da9a-1c27-4bcc-8f72-871bbff6acd1"
      },
      "outputs": [],
      "source": [
        "# Set the number of layers to freeze\n",
        "N = 124\n",
        "\n",
        "# Set the first N layers as non-trainable\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers[:N]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Print layer indices, names, and trainability status\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k7Ow8u8vHsnn"
      },
      "outputs": [],
      "source": [
        "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7_xqnKPHsnn",
        "outputId": "4640bda3-cf92-4a0d-d38e-4c6fa36aa052"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = X,          # Corretto: usa training set\n",
        "    y = y,          # Corretto: usa training set\n",
        "    batch_size = 32,\n",
        "    epochs = 30,          # Aumentato per dare più tempo al training\n",
        "    validation_data = (X, y),\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            patience=7,    # Aumentato per evitare stop prematuro\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tfk.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,    # Aumentato ma mantenuto minore dell'EarlyStopping\n",
        "            min_lr=1e-7,\n",
        "            verbose=1,\n",
        "            mode='min',\n",
        "            min_delta=1e-4,\n",
        "            cooldown=1\n",
        "        )\n",
        "    ]\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
        "ft_model.save(model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxRd-Pn7Hsnn"
      },
      "source": [
        "### Second Fine - Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8PcCtEoBHsnn",
        "outputId": "f1c706a9-7e9e-48eb-8646-ea76ac37a8ea"
      },
      "outputs": [],
      "source": [
        "# Re-load the model after transfer learning\n",
        "#ft_model = tfk.models.load_model('/content/Blood_Cells_MobileNetV3S_75.08.keras')\n",
        "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkXX9nrSHsnn",
        "outputId": "a7c088cc-7fad-482f-aa19-58126bd4183e"
      },
      "outputs": [],
      "source": [
        "# Set the MobileNetV3Small model layers as trainable\n",
        "ft_model.get_layer('convnext_xlarge').trainable = True\n",
        "\n",
        "# Set all MobileNetV3Small layers as non-trainable\n",
        "for layer in ft_model.get_layer('convnext_xlarge').layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
        "        layer.trainable = True\n",
        "        print(i, layer.name, type(layer).__name__, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YFgJNcLhHsnn",
        "outputId": "c72f82af-c2eb-48d5-995a-ac54435af745"
      },
      "outputs": [],
      "source": [
        "# Set the number of layers to freeze\n",
        "N = 100\n",
        "\n",
        "# Set the first N layers as non-trainable\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers[:N]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Print layer indices, names, and trainability status\n",
        "for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmqkBOPrHsnn"
      },
      "outputs": [],
      "source": [
        "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBlgqDz-Hsnn"
      },
      "outputs": [],
      "source": [
        "# Enable mixed precision\n",
        "tfk.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "kMSjUmUoHsnn",
        "outputId": "cf7c6f70-7d2b-4a36-cd69-ec22b5fd680d"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 5,\n",
        "    validation_data = (X_test, y_test),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=False),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,        # Riduzione graduale per un modello grande\n",
        "    patience=1,        # ~25-30% delle epoche totali\n",
        "    min_lr=1e-7,      # Considerando Adam come optimizer\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    min_delta=1e-4,   # Basato sulla scala delle tue loss\n",
        "    cooldown=1        # Breve periodo di stabilizzazione\n",
        ")]\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
        "ft_model.save(model_filename)\n",
        "\n",
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test_aug2)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test_aug2.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "\n",
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test_aug4)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test_aug3.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test_aug)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test_aug.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the test set and print a classification report\n",
        "y_pred = ft_model.predict(X_test_aug3)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
        "y_test_classes = y_test_aug3.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMEcNOVsHsno"
      },
      "source": [
        "### Submit Section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgrkr0cVHsno"
      },
      "outputs": [],
      "source": [
        "# file: model.py\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRs3RtsQHsno",
        "outputId": "f95efc23-2cd8-4a1a-9a7d-ceb21b1219c7"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the internal state of the model. Note that the __init__\n",
        "        method cannot accept any arguments.\n",
        "\n",
        "        The following is an example loading the weights of a pre-trained\n",
        "        model.\n",
        "        \"\"\"\n",
        "        self.neural_network = tfk.models.load_model('Blood_Cells_MobileNetV3S_99.12.keras')\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
        "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
        "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
        "        encoded.\n",
        "\n",
        "        The following is an example of a prediction from the pre-trained model\n",
        "        loaded in the __init__ method.\n",
        "        \"\"\"\n",
        "        preds = self.neural_network.predict(X)\n",
        "        if len(preds.shape) == 2:\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nse5OExwHsno",
        "outputId": "51050626-80eb-4685-b6dc-3f92a5e520fc"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
        "\n",
        "# Add files to the zip command if needed\n",
        "# The original path was incorrect. Using f-string to format correctly.\n",
        "!zip {filename} model.py Blood_Cells_MobileNetV3S_99.12.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-oMJvTazOTX"
      },
      "outputs": [],
      "source": [
        "!cp submission_241116_200916.zip /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hzABAuZdHsnh",
        "Sg9e8t0JHsni",
        "klXZpkQSHsnj",
        "RBcsepkVHsnl",
        "a7wYjeXNHsnl",
        "PnDRduxMHsnl",
        "Vvfy_vjwHsnn",
        "VDKxWaefHsnn"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
