{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Homework 2: Minimal Working Example\n",
    "\n",
    "To make your first submission, follow these steps:\n",
    "1. Create a folder named `[2024-2025] AN2DL/Homework 2` in your Google Drive.\n",
    "2. Upload the `mars_for_students.npz` file to this folder.\n",
    "3. Upload the Jupyter notebook `Homework 2 - Minimal Working Example.ipynb`.\n",
    "4. Load and process the data.\n",
    "5. Implement and train your model.\n",
    "6. Submit the generated `.csv` file to Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoKas3XOP2hg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    # Clear VRAM\n",
    "    tf.keras.backend.clear_session()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    \n",
    "    # Clear RAM\n",
    "    gc.collect()\n",
    "\n",
    "#This should clear the VRAM and RAM\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## üåê Connect Colab to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "f7a22749-4372-430d-8d40-59cdf209d7e7"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount(\"/gdrive\")\n",
    "#%cd /gdrive/My Drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ‚öôÔ∏è Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO6_Ft_8T56A",
    "outputId": "dda6f0d8-0c53-4931-d9cf-39bef0f643da"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras_cv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tfk.__version__}\")\n",
    "print(f\"GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ‚è≥ Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLaoDaG1V1Yg",
    "outputId": "dc88c5a2-4fc9-47f2-950c-89057ac74428"
   },
   "outputs": [],
   "source": [
    "data = np.load(\"Datasets/mars_filtered.npz\")\n",
    "# Access the keys correctly\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## üõ†Ô∏è Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmnTgJi_OOs1"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lABBQe2V3dVp",
    "outputId": "fb5162a5-5048-4a87-beff-2c887588f981"
   },
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are your image datasets\n",
    "# Add a channel dimension and normalize pixel values to [0, 1]\n",
    "X_train = X_train[..., np.newaxis] / 255.0\n",
    "X_val = X_val[..., np.newaxis] / 255.0\n",
    "\n",
    "# Calculate input shape and the number of unique classes in the labels\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (batch_size, 64, 128, 3)\n",
    "print(\"X_val shape:\", X_val.shape)      # Should be (batch_size, 64, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROpSvACB4Kh6",
    "outputId": "5f9586ff-8f83-491a-d9bf-d8e24e60ba64"
   },
   "outputs": [],
   "source": [
    "# Convert grayscale to RGB correctly\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)  # Ensure shape (batch_size, 64, 128, 3)\n",
    "X_val_rgb = np.repeat(X_val, 3, axis=-1)\n",
    "print(\"X_train_rgb shape:\", X_train_rgb.shape)  # Should be (batch_size, 64, 128, 3)\n",
    "print(\"X_test_rgb shape:\", X_val_rgb.shape)    # Should be (batch_size, 64, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vqu0EX7CVWPb",
    "outputId": "59bfdf60-3297-42ec-e992-8b8bb1a15631"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    UpSampling2D,\n",
    "    MaxPooling2D,\n",
    "    Concatenate,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Add\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "def conv_block(x, filters, initializer, name):\n",
    "    \"\"\"A convolutional block with two Conv2D layers, BatchNorm, and ReLU activation.\"\"\"\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=initializer, name=name + \"_conv1\")(x)\n",
    "    x = BatchNormalization(name=name + \"_bn1\")(x)\n",
    "    x = Activation(\"relu\", name=name + \"_act1\")(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=initializer, name=name + \"_conv2\")(x)\n",
    "    x = BatchNormalization(name=name + \"_bn2\")(x)\n",
    "    x = Activation(\"relu\", name=name + \"_act2\")(x)\n",
    "    return x\n",
    "\n",
    "def unet(input_shape, num_classes, name):\n",
    "    \"\"\"Builds a single UNet.\"\"\"\n",
    "    initializer = GlorotUniform()  # Xavier initialization\n",
    "    inputs = Input(shape=input_shape, name=name + \"_input\")\n",
    "\n",
    "    # Encoder Path\n",
    "    e1 = conv_block(inputs, 64, initializer, name=name + \"_encoder1\")\n",
    "    p1 = MaxPooling2D((2, 2), name=name + \"_pool1\")(e1)\n",
    "\n",
    "    e2 = conv_block(p1, 128, initializer, name=name + \"_encoder2\")\n",
    "    p2 = MaxPooling2D((2, 2), name=name + \"_pool2\")(e2)\n",
    "\n",
    "    e3 = conv_block(p2, 256, initializer, name=name + \"_encoder3\")\n",
    "    p3 = MaxPooling2D((2, 2), name=name + \"_pool3\")(e3)\n",
    "\n",
    "    e4 = conv_block(p3, 512, initializer, name=name + \"_encoder4\")\n",
    "    p4 = MaxPooling2D((2, 2), name=name + \"_pool4\")(e4)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = conv_block(p4, 1024, initializer, name=name + \"_bottleneck\")\n",
    "\n",
    "    # Decoder Path\n",
    "    d4 = UpSampling2D((2, 2), name=name + \"_up4\")(bn)\n",
    "    d4 = Concatenate(name=name + \"_concat4\")([d4, e4])\n",
    "    d4 = conv_block(d4, 512, initializer, name=name + \"_decoder4\")\n",
    "\n",
    "    d3 = UpSampling2D((2, 2), name=name + \"_up3\")(d4)\n",
    "    d3 = Concatenate(name=name + \"_concat3\")([d3, e3])\n",
    "    d3 = conv_block(d3, 256, initializer, name=name + \"_decoder3\")\n",
    "\n",
    "    d2 = UpSampling2D((2, 2), name=name + \"_up2\")(d3)\n",
    "    d2 = Concatenate(name=name + \"_concat2\")([d2, e2])\n",
    "    d2 = conv_block(d2, 128, initializer, name=name + \"_decoder2\")\n",
    "\n",
    "    d1 = UpSampling2D((2, 2), name=name + \"_up1\")(d2)\n",
    "    d1 = Concatenate(name=name + \"_concat1\")([d1, e1])\n",
    "    d1 = conv_block(d1, 64, initializer, name=name + \"_decoder1\")\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=\"softmax\", kernel_initializer=initializer, name=name + \"_output\")(d1)\n",
    "    return Model(inputs, outputs, name=name)\n",
    "\n",
    "def hierarchical_unet(input_shape, num_classes):\n",
    "    \"\"\"Creates the hierarchical UNet structure with coarse and fine segmentation.\"\"\"\n",
    "    # Coarse UNet\n",
    "    coarse_unet = unet(input_shape, num_classes, name=\"coarse_unet\")\n",
    "\n",
    "    # Fine UNet\n",
    "    coarse_output = coarse_unet.output\n",
    "    fine_input = Concatenate(name=\"fine_input_concat\")([coarse_output, coarse_unet.input])\n",
    "\n",
    "    # Adjust fine UNet to accept concatenated input\n",
    "    fine_input_shape = (input_shape[0], input_shape[1], input_shape[2] + num_classes)  # Adjusted shape\n",
    "    fine_unet_model = unet(fine_input_shape, num_classes, name=\"fine_unet\")\n",
    "    fine_unet = fine_unet_model(fine_input)\n",
    "\n",
    "    # Hierarchical Model\n",
    "    model = Model(inputs=coarse_unet.input, outputs=[coarse_output, fine_unet], name=\"hierarchical_unet\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the input shape and number of classes\n",
    "input_shape = (64, 128, 3)  # Example input size\n",
    "num_classes = 5  # Number of segmentation classes\n",
    "\n",
    "# Build the hierarchical UNet\n",
    "model = hierarchical_unet(input_shape, num_classes)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtVm9jJjArdI"
   },
   "outputs": [],
   "source": [
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "        if labels_to_exclude is None:\n",
    "            labels_to_exclude = [0]  # Default to excluding label 0\n",
    "        self.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert predictions to class labels\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "\n",
    "        # Flatten the tensors\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "        # Apply mask to exclude specified labels\n",
    "        for label in self.labels_to_exclude:\n",
    "            mask = tf.not_equal(y_true, label)\n",
    "            y_true = tf.boolean_mask(y_true, mask)\n",
    "            y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "        # Update the state\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"labels_to_exclude\": self.labels_to_exclude})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2-4C4RuWdyr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "# Define Mean IoU for evaluation\n",
    "def mean_iou_metric(num_classes):\n",
    "    def mean_iou(y_true, y_pred):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        metric = MeanIoU(num_classes=num_classes)\n",
    "        return metric(y_true, y_pred)\n",
    "    return mean_iou\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        \"coarse_unet_output\": \"sparse_categorical_crossentropy\",\n",
    "        \"fine_unet\": \"sparse_categorical_crossentropy\",\n",
    "    },\n",
    "    loss_weights={\"coarse_unet_output\": 0.4, \"fine_unet\": 0.6},\n",
    "    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q64l47T5hRv_"
   },
   "outputs": [],
   "source": [
    "class VisualizationCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_dataset, save_interval=5, num_samples=1):\n",
    "        super().__init__()\n",
    "        self.val_dataset = val_dataset.unbatch().take(num_samples)\n",
    "        self.save_interval = save_interval\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_interval == 0:\n",
    "            print(f\"\\nEpoch {epoch + 1}: Visualizing predictions...\\n\")\n",
    "            for idx, (inputs, labels) in enumerate(self.val_dataset):\n",
    "                # Extract inputs and labels\n",
    "                coarse_input = inputs[\"coarse_unet_input\"]\n",
    "                fine_input = inputs[\"fine_unet_input\"]\n",
    "                true_mask = labels[\"fine_unet\"]\n",
    "\n",
    "                # Generate predictions\n",
    "                pred_mask = self.model.predict({\"coarse_unet_input\": coarse_input[None], \"fine_unet_input\": fine_input[None]})\n",
    "                pred_mask = pred_mask[1]  # Extract the second output (fine_unet)\n",
    "\n",
    "                # Post-process predicted mask\n",
    "                pred_mask = np.argmax(pred_mask[0], axis=-1)  # Convert probabilities to class indices\n",
    "\n",
    "                # Plot the results\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(np.uint8(fine_input), interpolation=\"nearest\")\n",
    "                plt.title(\"Input Image\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(true_mask.numpy(), cmap=\"viridis\", interpolation=\"nearest\")\n",
    "                plt.title(\"True Mask\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(pred_mask, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "                plt.title(\"Predicted Mask\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.suptitle(f\"Sample {idx + 1}\")\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxJ9CA4GVovz",
    "outputId": "432cfe94-ac09-49f9-f511-01e90854f4e1"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# Spatial augmentations: Applied to both x and y\n",
    "spatial_augmentations = keras_cv.layers.Augmenter(\n",
    "    [\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "        keras_cv.layers.RandomRotation(factor=0.1),  # Rotate by ¬±10¬∞\n",
    "        keras_cv.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n",
    "        keras_cv.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        keras_cv.layers.RandomShear(x_factor=0.1, y_factor=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "color_augmentations = keras_cv.layers.Augmenter(\n",
    "    [\n",
    "\n",
    "        keras_cv.layers.AutoContrast(value_range=(0, 255)),   # Adjust contrast\n",
    "        keras_cv.layers.RandomSaturation(factor=0.2),  # Adjust saturation\n",
    "        keras_cv.layers.RandomHue(factor=0.2, value_range=(0, 255)),         # Adjust hue\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_augmentation = keras_cv.layers.Augmenter(\n",
    "    [\n",
    "        keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
    "        keras_cv.layers.RandomShear(x_factor=0.2, y_factor=0.2),\n",
    "        keras_cv.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        keras_cv.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def apply_combined_transform(x, y):\n",
    "    \"\"\"Apply spatial augmentations to both x and y, and color augmentations to x only.\"\"\"\n",
    "    # Resize y to match x (use nearest interpolation for segmentation masks)\n",
    "    y = tf.expand_dims(y, axis=-1)  # Add a channel dimension to y\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "\n",
    "    # Concatenate x and y along the channel dimension\n",
    "    concat = tf.concat([x, y], axis=-1)  # Shape: [batch_size, 64, 128, 4]\n",
    "\n",
    "    # Apply spatial augmentations to the combined tensor\n",
    "    augmented = spatial_augmentations(concat)\n",
    "\n",
    "    # Split the augmented tensor back into x and y\n",
    "    x_augmented = augmented[..., :3]  # First 3 channels are x (RGB)\n",
    "    y_augmented = augmented[..., 3:]  # Last channel is y (mask)\n",
    "    y_augmented = tf.squeeze(y_augmented, axis=-1)  # Remove the channel dimension from y\n",
    "\n",
    "    # Apply color augmentations to x only\n",
    "    x_augmented = color_augmentations(x_augmented)\n",
    "\n",
    "    return x_augmented, y_augmented\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train_rgb, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "        lambda x, y: (\n",
    "            {\"coarse_unet_input\": x, \"fine_unet_input\": x},  # Input keys matching model inputs\n",
    "            {\"coarse_unet_output\": y, \"fine_unet\": y},  # Output keys matching model outputs\n",
    "        ),\n",
    "        num_parallel_calls=AUTO,\n",
    "    )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val_rgb, y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "        lambda x, y: (\n",
    "            {\"coarse_unet_input\": x, \"fine_unet_input\": x},  # Input keys\n",
    "            {\"coarse_unet_output\": y, \"fine_unet\": y},  # Output keys\n",
    "        ),\n",
    "        num_parallel_calls=AUTO,\n",
    "    )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Add the callback to your training\n",
    "visualization_callback = VisualizationCallback(val_ds, save_interval=5, num_samples=3)\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=64,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_fine_unet_mean_iou', mode='max', patience=20, restore_best_weights=True),\n",
    "                 tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_fine_unet_mean_iou',\n",
    "    factor=0.5,        # Riduzione graduale per un modello grande\n",
    "    patience=5,        # ~25-30% delle epoche totali\n",
    "    min_lr=1e-7,      # Considerando Adam come optimizer\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=1e-4,   # Basato sulla scala delle tue loss\n",
    "    cooldown=1        # Breve periodo di stabilizzazione\n",
    "    ),\n",
    "    visualization_callback,]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtM0ubgdOzG-",
    "outputId": "8b46e4f8-94a8-4941-c277-7b151da43091"
   },
   "outputs": [],
   "source": [
    "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "model_filename = f\"model_{timestep_str}.keras\"\n",
    "model.save(model_filename)\n",
    "\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_val, y_train, y_val, X_train_rgb, X_val_rgb, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "z287uIQ_VGoK",
    "outputId": "0a461a94-c6a9-41c8-beee-8fce03a146f0"
   },
   "outputs": [],
   "source": [
    "data = np.load(\"Datasets/mars_for_students.npz\")\n",
    "X_test = data[\"test_set\"]\n",
    "X_test = X_test[..., np.newaxis] / 255.0\n",
    "X_test_rgb = np.repeat(X_test, 3, axis=-1)\n",
    "del data, X_test\n",
    "batch_size = 32  # Prova con un valore inferiore se necessario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test_rgb, batch_size=batch_size)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPjMEKqZW5jX"
   },
   "outputs": [],
   "source": [
    "def y_to_df(y) -> pd.DataFrame:\n",
    "    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "    n_samples = len(y)\n",
    "    y_flat = y.reshape(n_samples, -1)\n",
    "    df = pd.DataFrame(y_flat)\n",
    "    df[\"id\"] = np.arange(n_samples)\n",
    "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s18kX1uDconq"
   },
   "outputs": [],
   "source": [
    "# Create and download the csv submission file\n",
    "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
    "submission_filename = f\"submission_{timestep_str}.csv\"\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5yf7-lunye5"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download(submission_filename)\n",
    "\n",
    "#!cp submission_filename /content/drive/MyDrive/submissions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
