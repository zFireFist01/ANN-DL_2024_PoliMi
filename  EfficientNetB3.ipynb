{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    # Clear VRAM\n",
    "    tf.keras.backend.clear_session()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    \n",
    "    # Clear RAM\n",
    "    gc.collect()\n",
    "\n",
    "#This should clear the VRAM and RAM\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Existence and Status\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs TensorFlow detects\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"TensorFlow detected the following GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Name: {details['device_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#This is to check GPU-Status and Usage (works only for NVIDIA GPUs)\n",
    "!nvidia-smi\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tensorflow and Keras Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "from keras import layers as tfkl\n",
    "from keras import regularizers\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(tf.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline\n",
    "\n",
    "#Number of Classes in the Dataset\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to Load Data and load the datasets needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Load dataset from .npz file\n",
    "    data = np.load(path)\n",
    "    \n",
    "    # Trim dataset to the first 11959 entries and discard the rest\n",
    "    train_dataset = data['images'][:11959].copy()  # Copy to ensure no reference to the original array\n",
    "    test_dataset = data['labels'][:11959].copy()\n",
    "    \n",
    "    # Explicitly delete the original data to free up memory\n",
    "    del data\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test, y_test) = load_data(\"training_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test.shape)\n",
    "print(\"Test set shape (labels):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug, y_test_aug) = load_data(\"augmented_set.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load datafrom tfk.applications import ConvNeXt\n",
    "(X_test_aug2, y_test_aug2) = load_data(\"augmented_set2.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug2.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug3, y_test_aug3) = load_data(\"augmented_set3.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug3.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function and load data\n",
    "(X_test_aug4, y_test_aug4) = load_data(\"augmented_set4.npz\")\n",
    "\n",
    "print(\"Test set shape (images):\", X_test_aug4.shape)\n",
    "print(\"Test set shape (labels):\", y_test_aug4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Combination of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different type of concatenations of the datasets\n",
    "X_test_concat = np.concatenate((X_test, X_test_aug), axis=0)\n",
    "y_test_concat = np.concatenate((y_test, y_test_aug), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets \n",
    "X_test_concat2 = np.concatenate((np.concatenate((X_test, X_test_aug), axis=0), X_test_aug2), axis=0)\n",
    "y_test_concat2 = np.concatenate((np.concatenate((y_test, y_test_aug), axis=0), y_test_aug2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "X_test_concat3 = np.concatenate((X_test_concat, X_test_aug3), axis=0)\n",
    "y_test_concat3 = np.concatenate((y_test_concat, y_test_aug3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets -> All the datasets were made by me \n",
    "X_test_concat_d = np.concatenate((X_test_aug, X_test_aug3), axis=0)\n",
    "y_test_concat_d = np.concatenate((y_test_aug, y_test_aug3), axis=0)\n",
    "X_test_concat_d = np.concatenate((X_test_concat_d, X_test_aug4), axis=0)\n",
    "y_test_concat_d = np.concatenate((y_test_concat_d, y_test_aug4), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to conditionally one-hot encode a variable if it exists\n",
    "def conditional_one_hot_encode(var_name, num_classes):\n",
    "    if var_name in globals() and globals()[var_name] is not None:\n",
    "        globals()[var_name] = tfk.utils.to_categorical(globals()[var_name], num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Conditionally apply one-hot encoding to each variable\n",
    "conditional_one_hot_encode('y_test_aug', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug2', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug3', num_classes)\n",
    "conditional_one_hot_encode('y_test_aug4', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat2', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat3', num_classes)\n",
    "conditional_one_hot_encode('y_test_concat_d', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Ranger Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranger(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=1e-5, weight_decay=1e-4, sync_period=5, slow_step=0.5, name=\"Ranger\", **kwargs):\n",
    "        # Pass learning_rate directly to the base class initializer\n",
    "        super(Ranger, self).__init__(name=name, learning_rate=learning_rate, **kwargs)\n",
    "        \n",
    "        # Store other parameters\n",
    "        self.weight_decay = weight_decay\n",
    "        self.sync_period = sync_period\n",
    "        self.slow_step = slow_step\n",
    "        self.iterations = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "        \n",
    "        # Lookahead slow weights initialization\n",
    "        self.slow_weights = None\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, name=None, **kwargs):\n",
    "        # Use self.learning_rate directly in apply_gradients\n",
    "        for grad, var in grads_and_vars:\n",
    "            if grad is not None:\n",
    "                if self.weight_decay > 0:\n",
    "                    grad += self.weight_decay * var\n",
    "                var.assign_sub(self.learning_rate * grad)\n",
    "\n",
    "        # Initialize slow weights on the first call\n",
    "        if self.slow_weights is None:\n",
    "            self.slow_weights = [tf.Variable(v, trainable=False) for _, v in grads_and_vars]\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        self.iterations.assign_add(1)\n",
    "\n",
    "        # Apply Lookahead every sync_period steps using tf.cond\n",
    "        def apply_lookahead():\n",
    "            for slow_var, (_, var) in zip(self.slow_weights, grads_and_vars):\n",
    "                slow_var.assign_add(self.slow_step * (var - slow_var))\n",
    "                var.assign(slow_var)\n",
    "            return tf.no_op()  # no-op to satisfy return type requirement\n",
    "\n",
    "        # Check if the current iteration is a sync period\n",
    "        tf.cond(tf.equal(self.iterations % self.sync_period, 0), apply_lookahead, lambda: tf.no_op())\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Ranger, self).get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"sync_period\": self.sync_period,\n",
    "            \"slow_step\": self.slow_step\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Instantiate and use the custom Ranger optimizer\n",
    "optimizer = Ranger(learning_rate=1e-5, weight_decay=1e-4, sync_period=5, slow_step=0.5)\n",
    "optimizer_Adam = tfk.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The First - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = tfk.applications.EfficientNetB4(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),  # Typical input size for EfficientNetB4\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "# Specify 'to_file' argument with a path where you have write permissions\n",
    "tfk.utils.plot_model(mobilenet, to_file='/tmp/model.png', expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenet.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "\"\"\"\n",
    "# Definisci il pipeline completo di augmentazione\n",
    "augmentation = tf.keras.Sequential([\n",
    "    # Altre augmentazioni indipendenti\n",
    "    tfkl.RandomCrop(height=96, width=96),  # Regola la dimensione del crop se necessario\n",
    "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tfkl.RandomRotation(0.3),\n",
    "    tfkl.Dropout(0.1),\n",
    "    tfkl.Dropout(0.2),\n",
    "    tfkl.RandomContrast(0.3),\n",
    "    tfkl.RandomZoom(0.15),\n",
    "    tfkl.RandomBrightness(0.1),\n",
    "], name='advanced_preprocessing')\n",
    "\n",
    "\n",
    "#Apply the augmentation pipeline\n",
    "inputs = augmentation(inputs)\n",
    "\"\"\"\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenet(inputs)\n",
    "\n",
    "#x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dropout layer for regularization\n",
    "x = tfkl.Dropout(0.4, name='dropout')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "\n",
    "# Add layer normalizatiFinal_Project.ipynbon\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3'sdasdsadsa\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Lion optimiser\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=optimizer_Adam, metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "tl_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "import math\n",
    "\n",
    "class OneCycleScheduler(Callback):\n",
    "    \"\"\"\n",
    "    Implements the One Cycle Learning Rate Policy.\n",
    "\n",
    "    Args:\n",
    "        max_lr (float): The maximum learning rate.\n",
    "        total_steps (int): Total number of training steps (epochs * steps_per_epoch).\n",
    "        pct_start (float): Percentage of the cycle (in steps) spent increasing the learning rate.\n",
    "        div_factor (float): Determines the initial learning rate via initial_lr = max_lr / div_factor.\n",
    "        final_div_factor (float): Determines the minimum learning rate via min_lr = max_lr / final_div_factor.\n",
    "        anneal_strategy (str): Strategy for annealing ('cos' or 'linear').\n",
    "    \"\"\"\n",
    "    def __init__(self, max_lr, total_steps, pct_start=0.3, div_factor=25.0, final_div_factor=1e4, anneal_strategy='cos'):\n",
    "        super(OneCycleScheduler, self).__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.pct_start = pct_start\n",
    "        self.div_factor = div_factor\n",
    "        self.final_div_factor = final_div_factor\n",
    "        self.anneal_strategy = anneal_strategy\n",
    "        self.step_num = 0\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.total_steps is None:\n",
    "            self.total_steps = self.params['epochs'] * self.params['steps']\n",
    "        self.initial_lr = self.max_lr / self.div_factor\n",
    "        self.min_lr = self.max_lr / self.final_div_factor\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.step_num > self.total_steps:\n",
    "            return\n",
    "\n",
    "        # Calculate the current step's learning rate\n",
    "        if self.step_num < self.pct_start * self.total_steps:\n",
    "            # Increasing learning rate\n",
    "            pct = self.step_num / (self.pct_start * self.total_steps)\n",
    "            new_lr = self.initial_lr + pct * (self.max_lr - self.initial_lr)\n",
    "        else:\n",
    "            # Decreasing learning rate\n",
    "            pct = (self.step_num - self.pct_start * self.total_steps) / (self.total_steps * (1 - self.pct_start))\n",
    "            if self.anneal_strategy == 'cos':\n",
    "                new_lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(math.pi * pct))\n",
    "            elif self.anneal_strategy == 'linear':\n",
    "                new_lr = self.max_lr - pct * (self.max_lr - self.min_lr)\n",
    "            else:\n",
    "                raise ValueError(\"anneal_strategy must be 'cos' or 'linear'\")\n",
    "        \n",
    "        # Update the learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "        self.history.setdefault('lr', []).append(new_lr)\n",
    "\n",
    "        self.step_num += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 20\n",
    "steps_per_epoch = len(X_test_concat2) \n",
    "total_steps = epochs * steps_per_epoch\n",
    "\n",
    "one_cycle = OneCycleScheduler(\n",
    "    max_lr=1e-3,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "# Define other callbacks\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_lr=1e-6\n",
    ")\n",
    "# Train the model\n",
    "tl_history = tl_model.fit(\n",
    "    x=X_test_concat2,\n",
    "    y=y_test_concat2,\n",
    "    batch_size=64,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test_aug2 , y_test_aug2),\n",
    "    callbacks=[tfk.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        mode='max', patience=20,\n",
    "        restore_best_weights=True\n",
    "        ),  reduce_lr]\n",
    ").history\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "#model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
    "#tl_model.save(model_filename)\n",
    "\n",
    "# Save the trained model weights to a file with the accuracy included in the filename\n",
    "model_weights_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras.weights.h5'\n",
    "tl_model.save_weights(model_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
    "tl_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the First - Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = tl_model.predict(X_test_aug3)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "del tl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Fine - Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_51.02.keras')\n",
    "#ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_'+ str(final_val_accuracy) + '.keras')\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "#tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "ft_model.get_layer('convnext_small').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in ft_model.get_layer('convnext_small').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_small').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 153\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_small').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('convnext_small').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "ft_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = X_test_concat_d,\n",
    "    y = y_test_concat_d,\n",
    "    batch_size = 32,\n",
    "    epochs = 6,\n",
    "    validation_data = (X_test_aug2, y_test_aug2),\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "ft_model.save(model_filename)\n",
    "\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Fine - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('Blood_Cells_MobileNetV3S_76.54.keras')\n",
    "# Save the trained model weights to a file with the accuracy included in the filename\n",
    "model_weights_filename = 'Blood_Cells_MobileNetV3S_76.54.keras.weights.h5'\n",
    "ft_model.save_weights(model_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
    "mobilenetf = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=False,\n",
    "    classes=8,\n",
    "    classifier_activation=\"softmax\",\n",
    "    name=\"efficientnetv2-b3\",\n",
    ")\n",
    "\n",
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenetf.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenetf(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dropout layer for regularization\n",
    "x = tfkl.Dropout(0.8, name='dropout')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.8, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 un load_data(\"training_set.npz\")its and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.8, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tlf_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "tlf_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "\n",
    "# Load the saved weights\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_76.54.keras.weights.h5'  # replace <final_val_accuracy> with the actual accuracy\n",
    "tlf_model.load_weights(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MobileNetV3Small model layers as trainable\n",
    "tlf_model.get_layer('efficientnetv2-b3').trainable = True\n",
    "\n",
    "# Set all MobileNetV3Small layers as non-trainable\n",
    "for layer in tlf_model.get_layer('efficientnetv2-b3').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
    "for i, layer in enumerate(tlf_model.get_layer('efficientnetv2-b3').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 140\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(tlf_model.get_layer('efficientnetv2-b3').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(tlf_model.get_layer('efficientnetv2-b3').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "tlf_model.summary(expand_nested=True)\n",
    "\n",
    "# Display model architecture with layer shapes and trainable parameters\n",
    "tfk.utils.plot_model(tlf_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "tlf_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "tlf_history = tlf_model.fit(\n",
    "    x = X_test_concat2,\n",
    "    y = y_test_concat2,\n",
    "    batch_size = 32,\n",
    "    epochs = 3,\n",
    "    validation_data = (X_test_aug4, y_test_aug4),\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "tlf_model.save(model_filename)\n",
    "\n",
    "# Generate predictions on the test set and print a classification report\n",
    "y_pred = tlf_model.predict(X_test_aug3)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = y_test_aug3.argmax(axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "\n",
    "del tlf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
    "mobilenetf = tf.keras.applications.EfficientNetV2B3(\n",
    "    include_top=False,\n",
    "    input_shape=(96, 96, 3),\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=False,\n",
    "    classes=8,\n",
    "    classifier_activation=\"softmax\",\n",
    "    name=\"efficientnetv2-b3\",\n",
    ")\n",
    "\n",
    "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
    "mobilenetf.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
    "x = mobilenetf(inputs)\n",
    "\n",
    "x = tfkl.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# Add a batch normalization layer\n",
    "x = tfkl.BatchNormalization(name='batch_norm')(x)\n",
    "\n",
    "# Add a dropout layer for regularization\n",
    "x = tfkl.Dropout(0.4, name='dropout')(x)\n",
    "\n",
    "# Add a dense layer with 256 units and GELU activation\n",
    "x = tfkl.Dense(256, activation='gelu', name='dense1')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm1')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "# Add a second dense layer with 128 un load_data(\"training_set.npz\")its and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense2')(x)\n",
    "\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm2')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.4, name='dropout3')(x)\n",
    "\n",
    "# Add a third dense layer with 128 units and GELU activation\n",
    "x = tfkl.Dense(128, activation='gelu', name='dense3')(x)\n",
    "'''\n",
    "# Add layer normalization\n",
    "x = tfkl.LayerNormalization(name='layer_norm3')(x)\n",
    "\n",
    "# Add another dropout layer\n",
    "x = tfkl.Dropout(0.3, name='dropout4')(x)\n",
    "'''\n",
    "# Add final Dense layer for classification with softmax activation\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tlf_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
    "tlf_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "\n",
    "# Load the saved weights\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras.weights.h5'  # replace <final_val_accuracy> with the actual accuracy\n",
    "tlf_model.load_weights(model_filename)\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'Blood_Cells_MobileNetV3S_'+str(final_val_accuracy)+'.keras'\n",
    "tlf_model.save(model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('Blood_Cells_MobileNetV3S_91.82.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'Outputs/submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "# The original path was incorrect. Using f-string to format correctly.\n",
    "!zip {filename} model.py Blood_Cells_MobileNetV3S_91.82.keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
